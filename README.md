# Sales Forecasting with Machine Learning

**AUPP Machine Learning Final Project**  
Time Series Forecasting using XGBoost with Feature Engineering

[Research Paper](paper/main.md) | [Presentation Slides](presentation/slides.md) | [Project Structure](PROJECT_STRUCTURE.md) | [Setup Guide](QUICKSTART.md)

---

## Overview

This project implements multiple machine learning approaches for monthly sales forecasting, comparing traditional statistical methods, neural networks, and gradient boosting techniques. The goal was to develop an accurate forecasting model that could be used for business planning and inventory management.

## Key Results

- **Best Model**: XGBoost with engineered features
- **Accuracy**: 11.6% MAPE, 0.856 RÂ² score
- **Improvement**: 40% better than ensemble baseline
- **Dataset**: 48 months of e-commerce sales data (2014-2018)
- **Features**: 43 engineered features including lags, rolling statistics, and growth metrics

---

## ğŸ“ Project Structure

```
ML---Final-Project/
â”œâ”€â”€ ğŸ“„ README.md                       # Project overview
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md            # Detailed structure documentation
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ“„ setup.py                        # Package installation
â”œâ”€â”€ ğŸ“„ run_dashboard.sh                # Quick launch script
â”‚
â”œâ”€â”€ ğŸ“‚ data/                           # Dataset files
â”‚   â”œâ”€â”€ raw.csv                        # Original data (10K transactions)
â”‚   â”œâ”€â”€ cleaned.csv                    # Preprocessed data
â”‚   â””â”€â”€ featured.csv                   # Feature-engineered data
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                      # Jupyter notebooks
â”‚   â”œâ”€â”€ eda.ipynb                      # Exploratory Data Analysis
â”‚   â”œâ”€â”€ linear_regression.ipynb        # Baseline model
â”‚   â”œâ”€â”€ k_means_customer_segmentation.ipynb  # Customer clustering
â”‚   â””â”€â”€ predictive.ipynb               # Main forecasting models â­
â”‚
â”œâ”€â”€ ğŸ“‚ src/                            # Source code
â”‚   â””â”€â”€ evaluation/                    # Model evaluation scripts
â”‚       â”œâ”€â”€ xgboost_optimized.py      # Best model (11.6% MAPE)
â”‚       â”œâ”€â”€ feature_engineering.py     # 43 features creation
â”‚       â”œâ”€â”€ advanced_ensemble.py       # Prophet + LSTM ensemble
â”‚       â””â”€â”€ run_improvement_pipeline.py # Full pipeline
â”‚
â”œâ”€â”€ ğŸ“‚ results/                        # Model outputs
â”‚   â”œâ”€â”€ saved_models/                  # All trained models (.h5, .pkl)
â”‚   â”œâ”€â”€ production_model/              # XGBoost deployment package
â”‚   â””â”€â”€ xgboost_optimized/            # Performance results
â”‚
â”œâ”€â”€ ğŸ“‚ app/                            # Streamlit Dashboard (6 pages)
â”‚   â”œâ”€â”€ streamlit_app.py              # Main application
â”‚   â””â”€â”€ pages/                         # Dashboard pages
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                           # Documentation
â”‚   â”œâ”€â”€ figures/                       # Flowcharts (methodology, project)
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md           # Production deployment
â”‚   â””â”€â”€ ACCURACY_METRICS_GUIDE.md     # Metrics explanation
â”‚
â”œâ”€â”€ ğŸ“‚ paper/                          # Research deliverables
â”‚   â”œâ”€â”€ main.md                        # Academic paper (8,500 words)
â”‚   â””â”€â”€ Sale Forcasting - Final Project.pdf
â”‚
â”œâ”€â”€ ğŸ“‚ presentation/                   # Presentation materials
â”‚   â””â”€â”€ slides.md                      # Slide deck (21 slides)
â”‚
â””â”€â”€ ğŸ“‚ scripts/                        # Utility scripts
    â”œâ”€â”€ generate_flowchart.py
    â””â”€â”€ generate_methodology_flowchart.py
â”‚
â”œâ”€â”€ data/                            # Data files
â”‚   â”œâ”€â”€ raw.csv                      # Original dataset
â”‚   â””â”€â”€ cleaned.csv                  # Preprocessed data
â”‚
â”œâ”€â”€ results/                         # Model outputs
â”‚   â”œâ”€â”€ model_outputs/              # Saved models
â”‚   â”œâ”€â”€ metrics/                    # Performance metrics
â”‚   â””â”€â”€ visualizations/             # Generated plots
â”‚
â”œâ”€â”€ paper/                          # Research paper
â”‚   â”œâ”€â”€ main.md                     # Full paper (Markdown)
â”‚   â”œâ”€â”€ main.pdf                    # Full paper (PDF)
â”‚   â””â”€â”€ sections/                   # Paper sections
â”‚       â”œâ”€â”€ 01_abstract.md
â”‚       â”œâ”€â”€ 02_introduction.md
â”‚       â”œâ”€â”€ 03_methodology.md
â”‚       â”œâ”€â”€ 04_results.md
â”‚       â””â”€â”€ 05_conclusion.md
â”‚
â”œâ”€â”€ presentation/                   # Presentation materials
â”‚   â”œâ”€â”€ slides.md                   # Presentation slides
â”‚   â””â”€â”€ figures/                    # Presentation figures
â”‚
â”œâ”€â”€ reports/                        # Project reports
â”‚   â”œâ”€â”€ model_evaluation_report.md  # Comprehensive evaluation
â”‚   â””â”€â”€ executive_summary.md        # Non-technical summary
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ ENSEMBLE_MODEL_VALIDATION_GUIDE.md
â”‚   â”œâ”€â”€ ACCURACY_METRICS_GUIDE.md
â”‚   â””â”€â”€ API_DOCUMENTATION.md
â”‚
â””â”€â”€ app/                            # Streamlit dashboard
    â”œâ”€â”€ streamlit_app.py
    â””â”€â”€ pages/
```

---

## Getting Started

### Installation

```bash
# Clone repository
git clone https://github.com/Skylarrrolala/ML---Final-Project.git
cd ML---Final-Project

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# macOS users: Install OpenMP for XGBoost
brew install libomp
```

### Running the Project

**Jupyter Notebooks** (recommended for exploration):
```bash
jupyter notebook
# Open notebooks/predictive.ipynb
```

**Streamlit Dashboard**:
```bash
streamlit run app/streamlit_app.py
```

**Python Scripts**:
```bash
python src/evaluation/xgboost_optimized.py
```

---

## Dataset

**Source**: E-commerce sales transaction data  
**Period**: December 2014 - November 2018 (48 months)  
**Records**: ~10,000 transactions  
**Granularity**: Daily transactions aggregated to monthly sales

**Available Features**:
- Temporal: Order Date
- Financial: Sales, Quantity, Profit
- Product: Category, Sub-Category
- Geographic: Country, State, City, Region
- Customer: Customer Segment, Customer ID

---

## Methodology

### 1. Exploratory Data Analysis
Started with understanding the data through visualization and statistical analysis. Identified temporal trends, seasonal patterns, and sales distributions across different categories and regions. Also performed customer segmentation using K-Means clustering.

### 2. Baseline Models
Implemented linear regression as a baseline, using time-based features (month counters and seasonal indicators). This gave us MAPE of ~25% and RÂ² of ~0.65, establishing a benchmark for comparison.

### 3. Individual Models

**Facebook Prophet**
- Used for its strength in handling seasonality automatically
- Configured with multiplicative seasonality for yearly patterns
- Achieved 19.6% MAPE and 0.865 RÂ²

**LSTM Neural Network**
- Implemented with 50 units and 12-month sequence length
- Trained for 100 epochs using Adam optimizer
- Performance: 30.3% MAPE, 0.405 RÂ²

**Ensemble (Prophet + LSTM)**
- Combined the two models using weighted averaging (60% Prophet, 40% LSTM)
- Reasoning: leverage Prophet's seasonal expertise with LSTM's pattern learning
- Result: 15.2% MAPE, 0.826 RÂ²

### 4. Best Model: XGBoost with Feature Engineering

Created 43 features across five categories:
- **Lag features** (12): Historical values at 1, 3, 6, 12 month intervals
- **Rolling statistics** (12): Moving averages, standard deviations, min/max values
- **Date features** (7): Month, quarter, cyclical encodings
- **Growth metrics** (6): Month-over-month, year-over-year changes, momentum
- **Statistical features** (6): Z-scores, percentiles, deviations from mean

Configured XGBoost with:
- 100 trees, max depth of 4
- L1 and L2 regularization to prevent overfitting
- Cross-validation for hyperparameter tuning

**Final Performance**: 11.6% MAPE, 0.856 RÂ²

### 5. Validation

- Split data temporally (36 months training, 12 months testing)
- Performed 24-iteration walk-forward cross-validation
- Conducted statistical significance tests
- Checked residual diagnostics (normality, bias, autocorrelation)

---

## Results

### Model Performance Comparison

| Model | MAPE (%) | RÂ² Score | MAE ($) | Status |
|-------|----------|----------|---------|--------|
| Linear Regression | 25.3 | 0.653 | 18,234 | Baseline |
| Prophet | 21.6 | 0.820 | 15,234 | Good |
| LSTM | 32.6 | 0.760 | 18,923 | Fair |
| Ensemble (P+L) | 19.3 | 0.840 | 14,123 | Good |
| **XGBoost + Features** | **11.6** | **0.856** | **6,016** | **Excellent** |

### Analysis

XGBoost significantly outperformed other approaches. The 40% improvement over the ensemble model (from 19.3% to 11.6% MAPE) and 57% reduction in dollar error (from $14,123 to $6,016 MAE) demonstrates the value of systematic feature engineering.

Several factors contributed to XGBoost's success:
- Feature engineering transformed raw time series into rich tabular data
- Tree-based models handle tabular data better than neural networks in this case
- Regularization prevented overfitting despite limited training data (48 months)
- Training time was much faster (seconds vs hours for LSTM)

The five most important features were:
1. Number of orders (48.5% importance)
2. Volatility momentum (12.2%)
3. Sales percentile (9.8%)
4. Sales Z-score (7.7%)
5. 12-month lagged sales (3.6%)

This tells us that order frequency matters more than order size, and combining multiple feature types provides complementary predictive power.

---

## Academic Paper

The complete research paper includes background, literature review, detailed methodology, results analysis, and discussion of implications. Available in both markdown and PDF format in the `paper/` directory.

---

## Presentation

Presentation slides covering the problem statement, methodology, results, and business impact are available in `presentation/slides.md` (15-20 minutes).

---

## Technical Implementation

### Technologies Used
- **Python 3.11**: Core programming language
- **XGBoost**: Gradient boosting framework (primary model)
- **Prophet**: Time series forecasting (Facebook)
- **TensorFlow/Keras**: Deep learning (LSTM)
- **scikit-learn**: Machine learning utilities
- **pandas/numpy**: Data manipulation
- **matplotlib/seaborn**: Visualization
- **Streamlit**: Interactive dashboard

### Key Algorithms
1. **XGBoost**: Gradient boosting with regularization (best performance)
2. **Feature Engineering**: 43 features (lag, rolling, date, growth, statistical)
3. **Facebook Prophet**: Additive model (trend + seasonality + holidays)
4. **LSTM**: Recurrent neural network for sequence learning

---

## Usage Guide

**For reviewing the project:**
1. Read the research paper (`paper/main.md`)
2. Check presentation slides (`presentation/slides.md`)
3. Review the evaluation report (`reports/model_evaluation_report.md`)

**To explore the code:**
1. Start with the EDA notebook (`notebooks/eda.ipynb`)
2. Review the main predictive notebook (`notebooks/predictive.ipynb`)
3. Check the evaluation scripts (`src/evaluation/`)

**To reproduce results:**
1. Install dependencies (see Getting Started)
2. Run the notebooks in order
3. Execute evaluation scripts in `src/evaluation/`

**To use the trained models:**
1. Load models from `results/saved_models/`
2. Run predictions using scripts in `src/models/`
3. Launch the dashboard with `streamlit run app/streamlit_app.py`

---

## Business Value

### Use Cases
- Inventory planning: Forecast demand to optimize stock levels
- Revenue projection: Financial planning and budgeting
- Resource allocation: Staff scheduling for peak periods
- Marketing strategy: Time campaigns for high-demand periods

### Impact with XGBoost Model
With 11.6% MAPE, the model provides reliable forecasts for business decisions. For average monthly sales of $52,000, the Â±$6,000 error margin is manageable for:
- Inventory optimization (reduce overstock/stockouts)
- Tighter safety margins (Â±12% vs Â±20% previously)
- Better resource allocation
- More confident strategic planning

---

## Validation Summary

**Train/Test Split**: 36 months training, 12 months testing (temporal split)

**Cross-Validation**: 24-iteration walk-forward validation

**Statistical Tests**: 
- Paired t-tests confirmed XGBoost significantly outperforms individual models
- Friedman test (p = 0.016) shows statistically significant differences
- Residual diagnostics passed (normality, bias, autocorrelation checks)

**Uncertainty Quantification**: 95% confidence intervals provided with forecasts

Full validation methodology documented in `docs/ENSEMBLE_MODEL_VALIDATION_GUIDE.md`.

---

## Future Directions

Potential improvements we identified:
- Add external variables (holidays, promotions, economic indicators)
- Test newer architectures (Transformers, N-BEATS)
- Implement automated hyperparameter tuning
- Build a prediction API for production use
- Create category-specific and regional forecasting models
- Add multi-horizon forecasting (1, 3, 6, 12 months ahead)

---

## References

1. Taylor, S. J., & Letham, B. (2018). Forecasting at scale. *The American Statistician*.
2. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*.
3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*.

Documentation: [Prophet](https://facebook.github.io/prophet/), [TensorFlow LSTM](https://www.tensorflow.org/guide/keras/rnn), [scikit-learn](https://scikit-learn.org/stable/)

---

## Contributors

Dararithy Heng, Sivhuy Hong, Saifudine Lim, Someatra Pum

Machine Learning Final Project  
AUPP (American University of Phnom Penh), Fall 2025  
Instructor: Prof. Kuntha Pin

---

## License

This project is for academic purposes as part of the AUPP Machine Learning course.

---

## Contact

Email: hdararithy@gmail.com  
GitHub: @Skylarrrolala
```
